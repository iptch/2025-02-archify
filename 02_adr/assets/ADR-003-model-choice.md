# Using a LLM model via API vs self-hosting

## Date:
2025-02-15

## Status:
Accepted

## Context:
Given the large set of available LLM models, we need to choose which LLM to use and whether to use an API or self-host it, based on our requirements.
This decision impacts development velocity, operational costs and system (feedback provisioning and grading) performance.

The most crucial criteria for this decision are integration complexity, performance requirements, cost, context length capabilities and future migration paths (e.g. when switching the LLM provider). We need to evaluate open source solutions that can be locally deployed such as DeepSeek-R1/V1, Llama-3 or Mistral, or API services like OpenAI's GPT series, Anthropics Claude or Googles Gemini.

## Decision:
Decision is based on the following criteria
- Integration effort: Local LLM deployment requires significant DevOps expertise for model deployment, scaling and maintenance. API integration is straight forward, since this will only require API key handling and request handling.
- Performance: Performance considers the latency (with which models produce outputs), the quality of the model output, even though this is more difficult to quantify, and finally the scaling capabilities. Local LLMs achieve lower latency compared to API calls, but might have lower quality output for very advanced tasks (which require mathematical/logical thinking capabilities) compared to closed source state-of-the-art models. With the recent release of reasoning models, e.g. by DeepSeek, the quality of the outputs generated by open source models is comparable to closed source models as can be seen on the [Huggingface Leaderboard](https://lmarena.ai/), a well respected and widely used dashboard to compare the most recent LLM releases. In terms of performance, both local and API LLMs a reasonable choice. However, API services provide built-in scaling, which can be much more difficult with local LLMs.
- Usage cost: Local deployment requires a hardware infrastructure upfront cost, but much lower variable costs. API services charge based per-token.
![Comparison of LLM API usage costs](../../assets/images/cost-analysis.png "Cost Comparison")
At high volumes ( > millions of tokens/month), local deployments become more cost-efficient. A typical essay page contains about 500 words, i.e. about 600-700 tokens. Considering an exam requires 5 pages of written text, plus 10 pages of written context, one exam can require 10-15k tokens, which corresponds to 60-70 graded exams. Hence, 
- Context length: API services provide 32k-100k+ context windows and generally provide more flexibility for long-term content, while local LLM models require more careful prompt engineering
- Dependence on LLM provider:
- Migration effort
- 

To quantify the criteria better, we make use of the [Huggingface Leaderboard](https://lmarena.ai/), which ranks LLM capabilities by doing pairwise comparisons.
What is a decision?

### Why did we take this decision?

## Consequences:
- DevOps experts 

